{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Process explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly refer to the wrangle_act.ipynb for the codes for which the overall\n",
    "process is documented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim: \n",
    "To gather, assess and clean data to get some insights on the data and suggest the dog type which is on an average most sought after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering :\n",
    "    1. The twitter data enhanced.csv file was read into a dataframe                   twitter_archive using the pandas read_csv function.\n",
    "    2. The image predictions.tsv file was read using the http \n",
    "       requests function into the dataframe image_predictions.\n",
    "    3. The tweet_json file is read line by line directly by using the for \n",
    "    loop to get the tweet_id,retweet_count and favorite_count into a \n",
    "    dataframe df_tweets as I was not able to generate the access keys and           tokens because my application is still under review by the twitter \n",
    "    development team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess:\n",
    "    1. All the 3 dataframe values were assessed both visually \n",
    "       and programmatically.\n",
    "    2. Each of the dataframe were checked for Data Quality                            dimensions(Completeness,Validity,Accuracy and Consistency) \n",
    "       as well as Tidiness issues.\n",
    "    3. Visual assessment was by opening individual files and checking\n",
    "       for wrong data. As the tables were not very huge this was not a \n",
    "        huge prpblem.\n",
    "    4. Programmatic assessment was done using functions like head(),\n",
    "       info(),value_counts() etc..\n",
    "    5. Each of the assessments were then documented mentioning the data \n",
    "       quality dimensions and tidiness separately.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean:\n",
    "    1. The above documented assessments were picked one by one for \n",
    "     cleaning purposes.Before starting the cleaning process, copies of\n",
    "     each dataframe is made.\n",
    "    2. The cleaning process was divided into Define, Code and Test steps\n",
    "    for easier readability.\n",
    "    3. The Define steps to give solution to the earlier documented \n",
    "      assessment.\n",
    "    4. Code area is to perform the operation in order to achieve what is \n",
    "      mentioned in the define block.\n",
    "    5. Test portion is to test if we obtained the desired output as per the \n",
    "      define section.\n",
    "    6. The above steps were repeated for all the Data Quality Dimensions and \n",
    "       Tidiness issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store:\n",
    "    1. The cleaned(master) data is then stored into a new csv file.\n",
    "    2. This csv file is used for further visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
